import { Question } from '@/types';

// Questões realísticas baseadas em exemplos reais do exame DAA-C01
// Fonte: certificationpractice.com, vmexam.com, e outras fontes confiáveis
export const questionsExamRealistic: Question[] = [
  // ============================================
  // STORED PROCEDURES & TRANSACTIONS
  // ============================================
  {
    id: 'qer-1',
    topicId: 'topic-4-2',
    domainId: 'domain-4',
    question: 'Um analista de dados precisa criar um processo automatizado que primeiro insere dados resumidos em uma tabela de relatórios e depois deleta os registros de origem. A operação inteira deve ser atômica para prevenir duplicação ou perda de dados se um erro ocorrer. Qual abordagem usando stored procedure é mais apropriada?',
    options: [
      'Criar uma procedure que usa um bloco TRY...CATCH para fazer rollback manual do INSERT se o DELETE falhar',
      'Criar uma procedure que encapsula os statements INSERT e DELETE dentro de um bloco BEGIN...COMMIT transaction',
      'Criar duas procedures separadas, uma para INSERT e outra para DELETE, e chamá-las sequencialmente de uma Task',
      'Criar uma procedure que executa o INSERT, seguido do DELETE, e depende do auto-commit do Snowflake',
    ],
    correctAnswer: 1,
    explanation: 'Para garantir atomicidade, você deve encapsular múltiplos statements DML em um bloco de transação explícito usando BEGIN TRANSACTION e COMMIT. Isso garante que ambas as operações sejam commitadas juntas ou nenhuma seja.',
    difficulty: 'hard',
    tags: ['stored-procedures', 'transactions', 'atomicity'],
  },
  {
    id: 'qer-2',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Para investigar um declínio recente no engajamento de usuários em um app móvel, um analista de dados levanta a hipótese de que uma nova versão lançada em 2025-03-15 é a causa. O analista tem uma tabela USER_SESSIONS com colunas USER_ID, APP_VERSION e SESSION_DURATION_MINUTES. Qual query melhor isola o impacto da nova versão para validar essa hipótese?',
    options: [
      'SELECT APP_VERSION, SUM(SESSION_DURATION_MINUTES) FROM USER_SESSIONS GROUP BY APP_VERSION;',
      'SELECT APP_VERSION, AVG(SESSION_DURATION_MINUTES) FROM USER_SESSIONS GROUP BY APP_VERSION;',
      'SELECT USER_ID, AVG(SESSION_DURATION_MINUTES) FROM USER_SESSIONS GROUP BY USER_ID;',
      'SELECT COUNT(DISTINCT USER_ID) FROM USER_SESSIONS WHERE APP_VERSION = \'new_version\';',
    ],
    correctAnswer: 1,
    explanation: 'AVG(SESSION_DURATION_MINUTES) agrupado por APP_VERSION permite comparar diretamente o tempo médio de sessão entre versões, isolando o impacto da nova versão no engajamento.',
    difficulty: 'medium',
    tags: ['data-analysis', 'aggregation', 'diagnostics'],
  },
  {
    id: 'qer-3',
    topicId: 'topic-4-1',
    domainId: 'domain-4',
    question: 'Um analista observa latência significativa em um dashboard de vendas. O tile principal do dashboard consulta uma tabela de fatos muito grande (bilhões de linhas), realizando várias agregações (SUM, COUNT, AVG) agrupadas por categoria de produto e data. A tabela é atualizada com novos lotes de dados a cada 4 horas. Quais DUAS afirmações descrevem corretamente os benefícios de implementar uma Materialized View para este cenário? (Escolha duas)',
    options: [
      'Ela automaticamente e transparentemente mantém os dados agregados atualizados',
      'Ela elimina custos de storage associados aos dados agregados',
      'Ela melhora significativamente a performance das queries de agregação',
      'Ela pode ser definida usando window functions complexas como RANK() e QUALIFY',
      'Ela garante que queries do dashboard sempre acessem dados em tempo real',
    ],
    correctAnswer: 0, // Múltipla escolha: 0 e 2
    explanation: 'Materialized Views: (A) são automaticamente mantidas sincronizadas quando os dados base mudam, (C) melhoram dramaticamente a performance pré-computando agregações. MVs têm custo de storage, não suportam funções complexas como RANK(), e têm latência (não são tempo real).',
    difficulty: 'hard',
    tags: ['materialized-views', 'performance', 'dashboards'],
  },
  {
    id: 'qer-4',
    topicId: 'topic-5-3',
    domainId: 'domain-5',
    question: 'Um analista em uma empresa de e-commerce está diagnosticando um pico de devoluções de produtos de uma região específica. Eles têm acesso a um data share seguro de um parceiro de logística chamado LOGISTICS_DATA que contém uma view SHIPMENT_TRACKING. Como o analista pode coletar os dados de envio relevantes para correlacioná-los com a tabela interna RETURNS da empresa?',
    options: [
      'Criar um stream na tabela RETURNS para capturar mudanças e juntá-las ao share',
      'Escrever uma query que faz JOIN direto da tabela local RETURNS com a view compartilhada LOGISTICS_DATA.PUBLIC.SHIPMENT_TRACKING',
      'Usar o comando COPY para carregar a view SHIPMENT_TRACKING em uma tabela local',
      'Clonar o database LOGISTICS_DATA para fazer uma cópia local antes do JOIN',
    ],
    correctAnswer: 1,
    explanation: 'Data Shares permitem acesso direto em tempo real aos dados compartilhados sem necessidade de copiar. Você pode fazer JOIN diretamente com a view compartilhada como se fosse uma tabela local.',
    difficulty: 'medium',
    tags: ['data-sharing', 'joins', 'collaboration'],
  },
  {
    id: 'qer-5',
    topicId: 'topic-5-3',
    domainId: 'domain-5',
    question: 'Uma equipe de analytics de varejo precisa correlacionar seus dados de vendas internos, armazenados no Snowflake, com padrões históricos de clima para melhorar modelos de previsão de demanda. O requisito principal é acessar um dataset de clima de terceiros diretamente dentro do Snowflake, evitando a configuração e manutenção de um pipeline ETL tradicional. Qual é o método mais eficaz para atender a esse requisito?',
    options: [
      'Utilizar o Snowflake Marketplace para encontrar e montar um dataset de clima de um provider',
      'Desenvolver uma External Function para chamar uma API pública de clima para cada ponto de dado necessário',
      'Configurar Snowpipe para ingerir continuamente arquivos de clima entregues a um stage externo',
      'Construir uma UDF Python que faz scraping de informações de clima de vários websites públicos',
    ],
    correctAnswer: 0,
    explanation: 'O Snowflake Marketplace é a forma mais eficiente de acessar dados de terceiros. Datasets são montados diretamente na sua conta sem ETL, com atualizações automáticas pelo provider.',
    difficulty: 'easy',
    tags: ['marketplace', 'data-sharing', 'external-data'],
  },
  {
    id: 'qer-6',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Um analista em uma empresa de logística está construindo um modelo preditivo no Snowflake para estimar o tempo total de entrega baseado no número de pacotes em um envio. Para fazer previsões, o analista deve primeiro determinar os coeficientes da equação linear (y = mx + b). Quais das seguintes funções do Snowflake são fundamentais para calcular esses coeficientes específicos? (Escolha duas)',
    options: [
      'REGR_INTERCEPT',
      'CORR',
      'AVG',
      'STDDEV_SAMP',
      'REGR_SLOPE',
    ],
    correctAnswer: 0, // Múltipla escolha: 0 e 4
    explanation: 'REGR_SLOPE retorna o coeficiente m (inclinação) e REGR_INTERCEPT retorna b (intercepto) da equação de regressão linear y = mx + b. CORR mede correlação mas não fornece os coeficientes.',
    difficulty: 'hard',
    tags: ['regression', 'statistics', 'prediction'],
  },
  {
    id: 'qer-7',
    topicId: 'topic-4-1',
    domainId: 'domain-4',
    question: 'Um provider de dados está configurando um Snowflake Data Share e precisa selecionar o objeto apropriado para conceder acesso aos consumidores. Os objetivos do provider são fornecer uma representação lógica simplificada dos dados e impedir que consumidores façam engenharia reversa da lógica complexa de JOIN ou vejam como certos campos calculados são derivados. Quais DUAS características são oferecidas ao usar uma Secure View neste cenário de compartilhamento de dados? (Escolha duas)',
    options: [
      'Ela previne vazamento de dados que pode ocorrer através de detalhes de otimização de queries',
      'A definição da view é oculta dos consumidores de dados',
      'Os dados da view são automaticamente atualizados quando as tabelas base mudam',
      'A performance de queries da view é sempre mais rápida que uma view regular',
      'Ela reduz custos de storage comparado a compartilhar as tabelas base',
    ],
    correctAnswer: 0, // Múltipla escolha: 0 e 1
    explanation: 'Secure Views: (A) previnem inferência de dados através de otimizações do query planner, (B) ocultam a definição SQL de não-owners. Views regulares também atualizam automaticamente (C é falso). Secure views podem ser mais lentas (D é falso). Não afetam storage (E é falso).',
    difficulty: 'hard',
    tags: ['secure-view', 'data-sharing', 'security'],
  },
  {
    id: 'qer-8',
    topicId: 'topic-5-1',
    domainId: 'domain-5',
    question: 'Um analista de logística está construindo um dashboard operacional no Snowsight para monitorar performance de envios. O dashboard contém múltiplos gráficos, cada um derivado de uma query de worksheet separada contra diferentes tabelas (SHIPMENTS, CARRIER_PERFORMANCE). Um requisito chave é permitir que usuários não-técnicos filtrem interativamente todos os gráficos do dashboard simultaneamente por SHIPPING_REGION. Qual é o método mais eficaz para implementar essa funcionalidade?',
    options: [
      'Para cada worksheet, adicionar uma cláusula WHERE SHIPPING_REGION = :region_filter e instruir usuários a editar manualmente o SQL em cada tile para mudar o valor do filtro',
      'Criar um único filtro no nível do dashboard (:region_filter) e adicionar uma cláusula WHERE correspondente (WHERE SHIPPING_REGION = :region_filter) à query de cada tile que precisa ser filtrado',
      'Combinar todas as queries em um único worksheet complexo usando UNION ALL, aplicar um filtro de worksheet em SHIPPING_REGION, e adicionar este único tile ao dashboard',
      'Criar um filtro no nível do dashboard para a coluna SHIPPING_REGION. O filtro será automaticamente aplicado a qualquer tile cuja query inclua SHIPPING_REGION na lista SELECT',
    ],
    correctAnswer: 1,
    explanation: 'Filtros de dashboard usam parâmetros nomeados (como :region_filter) que podem ser referenciados em múltiplas queries de tiles. Você define o filtro uma vez no dashboard e adiciona WHERE clause em cada query que deve responder ao filtro.',
    difficulty: 'medium',
    tags: ['snowsight', 'dashboard', 'filters'],
  },
  {
    id: 'qer-9',
    topicId: 'topic-4-1',
    domainId: 'domain-4',
    question: 'Quando um usuário com a role JUNIOR_ANALYST visualiza um dashboard financeiro, um tile de scorecard mostrando "Valor Médio de Transação" exibe um valor significativamente menor do que quando um usuário com a role SENIOR_ANALYST vê o mesmo tile. O analista confirma que ambas as roles consultam a mesma view subjacente. Uma política de mascaramento dinâmico existe na coluna TRANSACTION_AMOUNT. Qual é a configuração mais provável da masking policy causando essa discrepância?',
    options: [
      'A política retorna um valor NULL para a role JUNIOR_ANALYST, que é excluído do cálculo AVG()',
      'A política retorna um valor fixo de 0 para a role JUNIOR_ANALYST, que é incluído no cálculo AVG()',
      'A política retorna um valor hasheado para a role JUNIOR_ANALYST, causando um erro de conversão de tipo',
      'A política retorna um número aleatório entre 1 e 10 para a role JUNIOR_ANALYST, diminuindo a média geral',
    ],
    correctAnswer: 1,
    explanation: 'Se a masking policy retorna 0 para JUNIOR_ANALYST, esses zeros são incluídos no AVG(), reduzindo a média calculada. NULLs seriam ignorados pelo AVG(). Valores hasheados causariam erro. Valores aleatórios também reduziriam mas de forma inconsistente.',
    difficulty: 'hard',
    tags: ['masking-policy', 'security', 'analytics'],
  },
  {
    id: 'qer-10',
    topicId: 'topic-3-2',
    domainId: 'domain-3',
    question: 'Consultar um grande dataset Parquet em um stage externo requer recuperar apenas as colunas event_timestamp e user_id. Para maximizar performance e reduzir scanning de dados, qual método deve ser empregado?',
    options: [
      'Carregar o dataset inteiro em uma tabela com uma coluna VARIANT e usar notação de ponto para extrair os campos necessários',
      'Consultar os arquivos Parquet staged diretamente, referenciando as colunas específicas usando a sintaxe $1:<column_name> na cláusula SELECT',
      'Usar a função FLATTEN nos arquivos staged para parsear a estrutura colunar antes de selecionar os campos requeridos',
      'Criar uma tabela temporária usando a função INFER_SCHEMA e depois consultar as duas colunas dessa tabela',
    ],
    correctAnswer: 1,
    explanation: 'Parquet é um formato colunar. Ao consultar diretamente do stage com $1:<column_name>, o Snowflake faz column pruning e lê apenas as colunas necessárias, minimizando I/O e maximizando performance.',
    difficulty: 'medium',
    tags: ['parquet', 'stages', 'query-optimization'],
  },
  {
    id: 'qer-11',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Para uma auditoria de compliance recorrente, um analista de dados deve extrair uma amostra de 10% da tabela CUSTOMER_FEEDBACK. Um requisito crítico é que o resultado da amostragem deve ser idêntico para cada execução, assumindo que os dados subjacentes não mudaram. Qual query satisfaz esse requisito de repetibilidade?',
    options: [
      'SELECT * FROM CUSTOMER_FEEDBACK SAMPLE (10);',
      'SELECT * FROM CUSTOMER_FEEDBACK QUALIFY ROW_NUMBER() OVER (ORDER BY RANDOM()) <= (SELECT COUNT(*) * 0.1 FROM CUSTOMER_FEEDBACK);',
      'SELECT * FROM CUSTOMER_FEEDBACK WHERE UNIFORM(1, 10, RANDOM()) = 1;',
      'SELECT * FROM CUSTOMER_FEEDBACK SAMPLE SYSTEM (10) SEED(2024);',
    ],
    correctAnswer: 3,
    explanation: 'SAMPLE SYSTEM com SEED garante resultados reproduzíveis. A seed fixa o gerador de números aleatórios, garantindo a mesma amostra em cada execução enquanto os dados não mudarem.',
    difficulty: 'medium',
    tags: ['sampling', 'reproducibility', 'compliance'],
  },
  {
    id: 'qer-12',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Um modelo simples de regressão linear está falhando em prever vendas mensais de varejo com precisão, provavelmente devido a fortes padrões sazonais. Um analista precisa melhorar a abordagem de previsão usando apenas as capacidades SQL built-in do Snowflake. Quais DUAS estratégias seriam mais eficazes para melhorar o poder preditivo do modelo? (Escolha duas)',
    options: [
      'Implementar uma UDF Python para rodar um modelo avançado de séries temporais como ARIMA',
      'Aumentar o parâmetro STATEMENT_TIMEOUT_IN_SECONDS para permitir mais tempo de computação das funções de regressão',
      'Usar REGR_R2 para confirmar a baixa precisão do modelo atual antes de fazer mudanças',
      'Usar a função CORR para identificar outras variáveis independentes em potencial que tenham relação linear mais forte com vendas',
      'Substituir a regressão simples por um cálculo de média móvel usando window functions como AVG(...) OVER (...) para suavizar os dados e capturar tendências',
    ],
    correctAnswer: 3, // Múltipla escolha: 3 e 4
    explanation: '(D) CORR ajuda a encontrar variáveis com melhor correlação linear. (E) Médias móveis com window functions capturam tendências sazonais suavizando a série temporal. REGR_R2 valida mas não melhora. UDF Python não é SQL built-in. Timeout não afeta precisão.',
    difficulty: 'hard',
    tags: ['regression', 'time-series', 'forecasting'],
  },
  {
    id: 'qer-13',
    topicId: 'topic-3-1',
    domainId: 'domain-3',
    question: 'Um file format reutilizável é necessário para arquivos CSV delimitados por pipe onde texto não está enclosed e "N/A" significa valor nulo. Qual statement SQL cria corretamente um named file format com essas especificações?',
    options: [
      'CREATE FILE FORMAT my_csv_format TYPE = CSV DELIMITER = \'|\' FIELD_OPTIONALLY_ENCLOSED_BY = \'NONE\' NULL_IF = \'N/A\';',
      'CREATE FILE FORMAT my_csv_format TYPE = \'CSV\' SEPARATOR = \'|\' SKIP_HEADER = 0 NULL_STRING = \'N/A\';',
      'CREATE FILE FORMAT my_csv_format TYPE = \'CSV\' FIELD_DELIMITER = \'|\' FIELD_OPTIONALLY_ENCLOSED_BY = NONE NULL_IF = (\'N/A\');',
      'CREATE FILE FORMAT my_csv_format TYPE = \'CSV\' FIELD_DELIMITER = \'|\' EMPTY_FIELD_AS_NULL = TRUE NULL_IF = (\'N/A\');',
    ],
    correctAnswer: 2,
    explanation: 'A sintaxe correta usa FIELD_DELIMITER para o separador, FIELD_OPTIONALLY_ENCLOSED_BY = NONE para indicar sem aspas, e NULL_IF = (\'N/A\') como tuple para strings que representam NULL.',
    difficulty: 'medium',
    tags: ['file-format', 'csv', 'data-loading'],
  },
  {
    id: 'qer-14',
    topicId: 'topic-4-2',
    domainId: 'domain-4',
    question: 'Para criar uma UDF em um schema específico e depois conceder a outra role a capacidade de usá-la, quais são os privilégios mínimos necessários? (Escolha duas)',
    options: [
      'Privilégio SELECT na UDF alvo',
      'USAGE no database e no schema onde a UDF irá residir',
      'CREATE FUNCTION no schema onde a UDF irá residir',
      'OWNERSHIP no schema onde a UDF irá residir',
    ],
    correctAnswer: 1, // Múltipla escolha: 1 e 2
    explanation: '(B) USAGE no database e schema são necessários para acessar objetos dentro deles. (C) CREATE FUNCTION no schema é necessário para criar a UDF. Para USAR a UDF, a outra role precisa de USAGE no schema + USAGE na UDF.',
    difficulty: 'medium',
    tags: ['udf', 'privileges', 'rbac'],
  },
  {
    id: 'qer-15',
    topicId: 'topic-2-1',
    domainId: 'domain-2',
    question: 'Limpar uma tabela TRANSACTIONS envolve converter uma coluna TRANSACTION_DATE VARCHAR para o tipo DATE. Esta coluna contém vários formatos de data e strings não-data como "pending". Qual query efetivamente realiza essa conversão, transformando valores não-parseáveis em NULL sem falhar?',
    options: [
      'SELECT CASE WHEN IS_DATE(TRANSACTION_DATE) THEN TO_DATE(TRANSACTION_DATE) ELSE NULL END FROM TRANSACTIONS;',
      'SELECT TO_DATE(TRANSACTION_DATE, \'AUTO\') FROM TRANSACTIONS;',
      'SELECT TRY_TO_DATE(TRANSACTION_DATE, \'AUTO\') FROM TRANSACTIONS;',
      'SELECT CAST(TRANSACTION_DATE AS DATE) FROM TRANSACTIONS;',
    ],
    correctAnswer: 2,
    explanation: 'TRY_TO_DATE retorna NULL para valores que não podem ser parseados como data, em vez de lançar erro. O formato \'AUTO\' tenta detectar automaticamente o formato da data. CAST e TO_DATE lançam erros em conversões inválidas.',
    difficulty: 'easy',
    tags: ['data-cleaning', 'type-conversion', 'try-functions'],
  },
  {
    id: 'qer-16',
    topicId: 'topic-5-2',
    domainId: 'domain-5',
    question: 'Um analista está configurando uma nova conexão JDBC para uma ferramenta de relatórios de terceiros e deve garantir que todas as queries dessa ferramenta usem o warehouse REPORTING_WH por padrão. Qual propriedade de conexão deve ser explicitamente incluída na string de conexão JDBC ou propriedades?',
    options: [
      'schema',
      'db',
      'role',
      'warehouse',
    ],
    correctAnswer: 3,
    explanation: 'A propriedade "warehouse" na connection string JDBC define o warehouse padrão para todas as queries. Sem ela, o usuário precisaria de um warehouse default configurado em seu profile.',
    difficulty: 'easy',
    tags: ['jdbc', 'connectivity', 'warehouse'],
  },
  {
    id: 'qer-17',
    topicId: 'topic-5-2',
    domainId: 'domain-5',
    question: 'Uma equipe de compliance financeiro requer uma notificação por email com um PDF do dashboard "Transaction Monitoring", mas apenas quando o valor no tile de scorecard "High-Risk Transactions" ultrapassar 100. Como um analista de dados pode configurar esse requisito específico de alertas dentro do Snowsight?',
    options: [
      'Configurar um Snowflake Alert que dispara quando a contagem de transações de alto risco excede 100, depois vincular o alert à subscription do dashboard',
      'Configurar uma subscription de email do dashboard para rodar em um schedule, e definir a condição para enviar apenas quando a query de um tile específico retornar resultados',
      'Criar uma Snowflake Task que roda uma query na tabela fonte do dashboard e usa SYSTEM$SEND_EMAIL() se a condição for atendida',
      'Compartilhar o dashboard com a equipe de compliance e instruí-los a definir uma regra de notificação pessoal no tile específico',
    ],
    correctAnswer: 2,
    explanation: 'Snowflake Tasks combinadas com SYSTEM$SEND_EMAIL() (via notification integration) permitem criar alertas programáticos baseados em condições de query. Alerts nativos do Snowflake também funcionam mas Tasks oferecem mais flexibilidade.',
    difficulty: 'hard',
    tags: ['alerts', 'notifications', 'tasks'],
  },
  {
    id: 'qer-18',
    topicId: 'topic-6-3',
    domainId: 'domain-6',
    question: 'Para investigar por que vários dashboards chave têm performado lentamente desde um resize recente de warehouse, um analista de dados precisa coletar dados sobre tempos de execução de queries e carga do warehouse. Qual abordagem fornece o acesso mais direto a esses dados históricos de performance para análise diagnóstica no Snowflake?',
    options: [
      'Consultar as views apropriadas dentro do schema SNOWFLAKE.ACCOUNT_USAGE',
      'Usar a função GET_DDL para revisar o histórico de configuração do warehouse',
      'Habilitar o parâmetro AUTO_SUSPEND e monitorar o status do warehouse',
      'Consultar as views de monitoramento em tempo real no INFORMATION_SCHEMA',
    ],
    correctAnswer: 0,
    explanation: 'SNOWFLAKE.ACCOUNT_USAGE contém views como QUERY_HISTORY, WAREHOUSE_METERING_HISTORY, e WAREHOUSE_LOAD_HISTORY com dados históricos de até 365 dias. INFORMATION_SCHEMA é em tempo real e não mantém histórico extenso.',
    difficulty: 'medium',
    tags: ['account-usage', 'performance', 'diagnostics'],
  },
  {
    id: 'qer-19',
    topicId: 'topic-5-1',
    domainId: 'domain-5',
    question: 'Qual das seguintes é uma característica chave que um analista de dados pode configurar para um tile de Scorecard dentro de um dashboard Snowsight?',
    options: [
      'Adicionar um filtro de texto que se aplica apenas a esse tile específico de scorecard',
      'Exibir um valor primário de uma coluna e um valor de comparação de outra coluna',
      'Configurar formatação condicional para mudar a cor baseada no valor',
      'Mostrar um sparkline para visualizar a tendência do valor primário ao longo do tempo',
    ],
    correctAnswer: 1,
    explanation: 'Scorecards no Snowsight suportam valor primário e valor de comparação (para mostrar variação/tendência). Não suportam formatação condicional de cores, filtros individuais por tile, ou sparklines nativamente.',
    difficulty: 'medium',
    tags: ['snowsight', 'scorecard', 'visualization'],
  },
  {
    id: 'qer-20',
    topicId: 'topic-2-3',
    domainId: 'domain-2',
    question: 'Construir uma camada de consumo para uma ferramenta de BI com performance de JOIN ruim requer otimização para velocidade em dashboards predefinidos. Para alcançar os tempos de resposta mais rápidos para agregações de vendas, qual estrutura de dados é mais apropriada?',
    options: [
      'Uma série de secure views que fazem JOIN das tabelas fonte normalizadas em tempo real',
      'Um star schema com uma tabela fato central e múltiplas tabelas dimensão',
      'Uma tabela desnormalizada (flattened) ampla contendo todos os atributos requeridos',
      'Um snowflake schema para minimizar custos de storage através de normalização',
    ],
    correctAnswer: 2,
    explanation: 'Para ferramentas de BI com performance ruim de JOIN, uma tabela desnormalizada (wide/flat) elimina a necessidade de JOINs em runtime, fornecendo os tempos de resposta mais rápidos às custas de redundância de dados.',
    difficulty: 'medium',
    tags: ['data-modeling', 'denormalization', 'bi-optimization'],
  },

  // ============================================
  // QUESTÕES ADICIONAIS DO SYLLABUS OFICIAL
  // ============================================
  {
    id: 'qer-21',
    topicId: 'topic-1-1',
    domainId: 'domain-1',
    question: 'Ao planejar a coleta de volume de dados, qual é uma consideração importante para garantir escalabilidade e performance?',
    options: [
      'A localização física dos dados',
      'Os tipos de ferramentas de visualização de dados usadas',
      'As capacidades de processamento de dados',
      'A taxa de crescimento esperada dos dados',
    ],
    correctAnswer: 3,
    explanation: 'A taxa de crescimento esperada dos dados é crítica para planejamento de escalabilidade. Ela afeta decisões sobre particionamento, clustering, sizing de warehouse, e estratégias de arquivamento.',
    difficulty: 'easy',
    tags: ['planning', 'scalability', 'data-volume'],
  },
  {
    id: 'qer-22',
    topicId: 'topic-5-1',
    domainId: 'domain-5',
    question: 'Configurar subscriptions e updates em uma ferramenta de dashboard primariamente ajuda em:',
    options: [
      'Mudar o layout do dashboard',
      'Reduzir o tamanho dos dados',
      'Garantir que usuários recebam os dados mais recentes',
      'Melhorar a segurança dos dados',
    ],
    correctAnswer: 2,
    explanation: 'Subscriptions e scheduled updates garantem que stakeholders recebam automaticamente os dados mais atualizados sem precisar acessar o dashboard manualmente.',
    difficulty: 'easy',
    tags: ['dashboard', 'subscriptions', 'automation'],
  },
  {
    id: 'qer-23',
    topicId: 'topic-5-3',
    domainId: 'domain-5',
    question: 'Qual é uma consideração crítica ao usar data shares no Snowflake para fazer JOIN de dados com datasets existentes?',
    options: [
      'Garantir compatibilidade de schema dos dados',
      'O número de likes na página de mídia social do Snowflake',
      'O apelo visual dos dados',
      'A marca da ferramenta de visualização de dados',
    ],
    correctAnswer: 0,
    explanation: 'Compatibilidade de schema é essencial para JOINs bem-sucedidos. Tipos de dados, nomes de colunas e relacionamentos devem ser compatíveis entre os dados compartilhados e locais.',
    difficulty: 'easy',
    tags: ['data-sharing', 'schema', 'joins'],
  },
  {
    id: 'qer-24',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Um aspecto chave de realizar análises exploratórias ad-hoc é:',
    options: [
      'Seguir um modelo de dados estrito',
      'Depender exclusivamente de hipóteses predefinidas',
      'Flexibilidade em queries e exploração de dados',
      'Limitar fontes de dados',
    ],
    correctAnswer: 2,
    explanation: 'Análise exploratória ad-hoc requer flexibilidade para explorar dados livremente, testar hipóteses rapidamente, e descobrir insights não planejados sem restrições de modelos rígidos.',
    difficulty: 'easy',
    tags: ['exploratory-analysis', 'ad-hoc', 'flexibility'],
  },
  {
    id: 'qer-25',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Qual abordagem é mais adequada para fazer previsões baseadas em dados?',
    options: [
      'Queries SQL básicas',
      'Usar funções SQL built-in para análise estatística',
      'Depender apenas de fontes de dados externas',
      'Ignorar tendências históricas de dados',
    ],
    correctAnswer: 1,
    explanation: 'Snowflake oferece funções estatísticas built-in (REGR_SLOPE, REGR_INTERCEPT, CORR, etc.) que permitem fazer previsões diretamente em SQL sem necessidade de ferramentas externas.',
    difficulty: 'easy',
    tags: ['prediction', 'statistics', 'sql-functions'],
  },
  {
    id: 'qer-26',
    topicId: 'topic-3-2',
    domainId: 'domain-3',
    question: 'Por que o formato Parquet é preferido para datasets complexos?',
    options: [
      'Ele tem apresentação visual de dados atraente',
      'Ele suporta esquemas de compressão e encoding eficientes',
      'Ele aleatoriamente altera dados para testes',
      'Ele muda cores de dados para diferenciação',
    ],
    correctAnswer: 1,
    explanation: 'Parquet é um formato colunar que suporta compressão eficiente (como Snappy, Gzip), encoding otimizado por tipo de dado, e column pruning - tornando-o ideal para datasets analíticos grandes e complexos.',
    difficulty: 'easy',
    tags: ['parquet', 'file-format', 'compression'],
  },
  {
    id: 'qer-27',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Data clustering é um exemplo de qual tipo de técnica de análise de dados?',
    options: [
      'Análise descritiva',
      'Análise exploratória',
      'Análise prescritiva',
      'Análise preditiva',
    ],
    correctAnswer: 3,
    explanation: 'Clustering é uma técnica de análise preditiva/prescritiva usada para segmentar dados em grupos baseados em similaridades, permitindo previsões sobre comportamento de novos dados.',
    difficulty: 'medium',
    tags: ['clustering', 'predictive-analysis', 'data-science'],
  },
  {
    id: 'qer-28',
    topicId: 'topic-4-1',
    domainId: 'domain-4',
    question: 'Quais são fatores importantes a considerar ao criar tabelas e views no Snowflake? (Escolha três)',
    options: [
      'Segurança de dados e controles de acesso',
      'A cor das tabelas e views',
      'Otimização para performance de queries',
      'Documentação e gestão de metadados',
    ],
    correctAnswer: 0, // Múltipla: A, C, D
    explanation: 'Ao criar tabelas/views: (A) configure RBAC e políticas de segurança apropriadas, (C) considere clustering keys e materialized views para performance, (D) use COMMENT e tags para documentação e governança.',
    difficulty: 'medium',
    tags: ['tables', 'views', 'best-practices'],
  },
  {
    id: 'qer-29',
    topicId: 'topic-2-1',
    domainId: 'domain-2',
    question: 'Qual dos seguintes é um passo chave na preparação de dados?',
    options: [
      'Normalização de dados',
      'Deploy de modelo',
      'Seleção de algoritmo',
      'Análise visual',
    ],
    correctAnswer: 0,
    explanation: 'Normalização de dados (padronização de formatos, tratamento de NULLs, conversão de tipos) é um passo fundamental na preparação de dados antes de análise ou modelagem.',
    difficulty: 'easy',
    tags: ['data-preparation', 'normalization', 'etl'],
  },
  {
    id: 'qer-30',
    topicId: 'topic-2-4',
    domainId: 'domain-2',
    question: 'Identificar demographics e relacionamentos em um dataset é crucial para:',
    options: [
      'Limpeza de dados',
      'Tratamento de erros',
      'Modelagem de dados',
      'Normalização de database',
    ],
    correctAnswer: 2,
    explanation: 'Entender demographics (características dos dados) e relacionamentos é fundamental para modelagem de dados - definindo entidades, atributos, e como os dados se relacionam.',
    difficulty: 'easy',
    tags: ['data-modeling', 'relationships', 'demographics'],
  },
];

